{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09ea3cc-7e9d-408e-a14f-d05ad517701b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q chromadb openai pydantic sentence-transformers orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb071e6-0407-41a9-b2ea-bd72f298b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import orjson\n",
    "import logging\n",
    "from typing import Dict, List, Literal, Optional\n",
    "import numpy as np\n",
    "from chromadb import AsyncHttpClient, Settings\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import functools\n",
    "import json\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "QWEN_MODEL = \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\"\n",
    "vllm_client = AsyncOpenAI(\n",
    "    base_url=\"http://65.109.137.0:60564/v1\", api_key=\"dummy_key\"\n",
    ")\n",
    "\n",
    "embedding_model = SentenceTransformer(\"cointegrated/LaBSE-en-ru\")\n",
    "\n",
    "chroma_client = await AsyncHttpClient(host=\"91.184.242.207\", port=8000, settings=Settings(anonymized_telemetry=False))\n",
    "collection = await chroma_client.get_or_create_collection(\"documents\")\n",
    "\n",
    "def create_embeddings(texts: List[str]):\n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    logger.debug(f\"Created embeddings for texts: {texts}\")\n",
    "    return embeddings\n",
    "\n",
    "@functools.lru_cache(maxsize=1000)\n",
    "def create_cached_embeddings(text: str) -> np.ndarray:\n",
    "    return create_embeddings([text])[0]\n",
    "\n",
    "class SourceReference(BaseModel):\n",
    "    document_title: str = Field(..., description=\"Title of the referenced document\")\n",
    "    section: str = Field(..., description=\"Section number or identifier\")\n",
    "    exact_quote: str = Field(..., description=\"Direct quote from the source\")\n",
    "    relevance: Literal[\"high\", \"medium\", \"low\"] = Field(..., description=\"Relevance level of the reference\")\n",
    "\n",
    "class ThinkStep(BaseModel):\n",
    "    reasoning: str = Field(..., description=\"Step-by-step thought process\")\n",
    "    conclusion: str = Field(..., description=\"Intermediate or final conclusion\")\n",
    "\n",
    "class Checklist(BaseModel):\n",
    "    query_understood: bool = Field(..., description=\"Query is fully understood\")\n",
    "    context_analyzed: bool = Field(..., description=\"Relevant context found and analyzed\")\n",
    "    sources_verified: bool = Field(..., description=\"Sources properly referenced\")\n",
    "    reasoning_complete: bool = Field(..., description=\"Full analysis conducted\")\n",
    "    answer_validated: bool = Field(..., description=\"Answer checked for accuracy\")\n",
    "    additional_notes: Optional[str] = Field(None, description=\"Any additional verification notes\")\n",
    "\n",
    "    @field_validator(\"additional_notes\", mode=\"before\")\n",
    "    def validate_notes(cls, value):\n",
    "        if isinstance(value, bool):\n",
    "            return None\n",
    "        return value\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    source_references: List[SourceReference] = Field(..., description=\"List of relevant source references\")\n",
    "    thinking_steps: List[ThinkStep] = Field(..., description=\"Chain of reasoning steps\")\n",
    "    brief_answer: str = Field(..., description=\"Concise answer to the query\")\n",
    "    detailed_answer: Optional[str] = Field(None, description=\"Detailed explanation if needed\")\n",
    "    checklist: Checklist = Field(..., description=\"Validation checklist\")\n",
    "\n",
    "class Prompts:\n",
    "    SYSTEM = \"\"\"\n",
    "    <system>\n",
    "        <task>\n",
    "            <primary>Process documentation queries with structured output</primary>\n",
    "            <approach>Chain-of-thought reasoning with validation checklist</approach>\n",
    "        </task>\n",
    "        \n",
    "        <rules>\n",
    "            <output>\n",
    "                <format>Strict JSON adherence to provided schema</format>\n",
    "                <validation>Must pass all checklist items</validation>\n",
    "            </output>\n",
    "            <reasoning>\n",
    "                <steps>Sequential thought process</steps>\n",
    "                <verification>Cross-reference with documentation</verification>\n",
    "            </reasoning>\n",
    "        </rules>\n",
    "        \n",
    "        <quality>\n",
    "            <accuracy>Verify against source material</accuracy>\n",
    "            <completeness>Address all question aspects</completeness>\n",
    "            <structure>Follow provided schema exactly</structure>\n",
    "        </quality>\n",
    "    </system>\n",
    "    \"\"\"\n",
    "\n",
    "    CONTEXT = \"\"\"\n",
    "    <context>\n",
    "        <documentation>\n",
    "            {context_text}\n",
    "        </documentation>\n",
    "        <schema>\n",
    "            {schema_definition}\n",
    "        </schema>\n",
    "        <query>{question}</query>\n",
    "    </context>\n",
    "    \"\"\"\n",
    "\n",
    "    CLARIFICATION = \"\"\"\n",
    "    <clarification>\n",
    "        <task>\n",
    "            <primary>Determine if query needs clarification</primary>\n",
    "            <output_format>Single question or \"No clarification needed\"</output_format>\n",
    "        </task>\n",
    "        \n",
    "        <rules>\n",
    "            <analysis>\n",
    "                <check>Query completeness</check>\n",
    "                <check>Technical specificity</check>\n",
    "                <check>Context sufficiency</check>\n",
    "            </analysis>\n",
    "            <response>\n",
    "                <format>Clear, specific question</format>\n",
    "                <language>English</language>\n",
    "            </response>\n",
    "        </rules>\n",
    "        \n",
    "        <validation>\n",
    "            <condition>If query is clear and complete</condition>\n",
    "            <response>\"No clarification needed\"</response>\n",
    "        </validation>\n",
    "    </clarification>\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_schema_definition() -> str:\n",
    "        \"\"\"Convert Pydantic model to OpenAI-compatible JSON schema\"\"\"\n",
    "        schema = Answer.model_json_schema()\n",
    "        # Remove unnecessary fields from schema\n",
    "        for key in ['title', 'description']:\n",
    "            schema.pop(key, None)\n",
    "        return json.dumps(schema, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def format_context(cls, context_text: str, question: str) -> str:\n",
    "        return cls.CONTEXT.format(\n",
    "            context_text=context_text,\n",
    "            schema_definition=cls.get_schema_definition(),\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "def validate_clarification(clarification: str) -> bool:\n",
    "    if not clarification.endswith('?') and clarification != \"No clarification needed\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def clean_response(response_text: str) -> str:\n",
    "    return \"\".join(c for c in response_text if c.isprintable() or c in \"\\n\\t\")\n",
    "\n",
    "def create_error_answer(error_message: str) -> Answer:\n",
    "    return Answer(\n",
    "        source_references=[],\n",
    "        thinking_steps=[],\n",
    "        brief_answer=f\"Error processing response: {error_message}\",\n",
    "        detailed_answer=None,\n",
    "        checklist=Checklist(\n",
    "            query_understood=False,\n",
    "            context_analyzed=False,\n",
    "            sources_verified=False,\n",
    "            reasoning_complete=False,\n",
    "            answer_validated=False,\n",
    "            additional_notes=None\n",
    "        )\n",
    "    )\n",
    "\n",
    "async def get_relevant_documents(query: str) -> List[Dict]:\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "\n",
    "    results = await collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=200,\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "    )\n",
    "\n",
    "    relevant_docs = []\n",
    "    seen_sections = set()\n",
    "\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        section_number = meta.get(\"section_number\")\n",
    "        if section_number not in seen_sections:\n",
    "            relevant_docs.append({\n",
    "                \"content\": doc,\n",
    "                \"metadata\": meta,\n",
    "                \"section_number\": section_number\n",
    "            })\n",
    "            seen_sections.add(section_number)\n",
    "\n",
    "            if section_number:\n",
    "                for i in range(-1, 2):\n",
    "                    nearby_section = section_number + i\n",
    "                    if nearby_section not in seen_sections:\n",
    "                        for nearby_doc, nearby_meta in zip(\n",
    "                            results[\"documents\"][0],\n",
    "                            results[\"metadatas\"][0]\n",
    "                        ):\n",
    "                            if nearby_meta.get(\"section_number\") == nearby_section:\n",
    "                                relevant_docs.append({\n",
    "                                    \"content\": nearby_doc,\n",
    "                                    \"metadata\": nearby_meta,\n",
    "                                    \"section_number\": nearby_section\n",
    "                                })\n",
    "                                seen_sections.add(nearby_section)\n",
    "                                break\n",
    "\n",
    "    return relevant_docs[:15]\n",
    "\n",
    "async def generate_clarifying_question(original_question: str) -> str:\n",
    "    response = await vllm_client.chat.completions.parse(\n",
    "        model=QWEN_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": Prompts.CLARIFICATION},\n",
    "            {\"role\": \"user\", \"content\": f\"Query: {original_question}\"}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=256,\n",
    "        response_format={\"type\": \"text\"}\n",
    "    )\n",
    "    return response.strip()\n",
    "\n",
    "async def ask_question(question: str) -> Answer:\n",
    "    try:\n",
    "        context = await get_relevant_documents(question)\n",
    "        context_text = \"\\n\".join(\n",
    "            f\"Section {doc['section_number']}: {doc['content']}\" \n",
    "            for doc in context\n",
    "        )\n",
    "\n",
    "        response = await vllm_client.chat.completions.parse(\n",
    "            model=QWEN_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": Prompts.SYSTEM},\n",
    "                {\"role\": \"user\", \"content\": Prompts.format_context(\n",
    "                    context_text=context_text,\n",
    "                    question=question\n",
    "                )}\n",
    "            ],\n",
    "            response_format=Answer,\n",
    "            temperature=0.3,\n",
    "            max_tokens=2048\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in ask_question: {str(e)}\")\n",
    "        return create_error_answer(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018b464-5939-46e7-b589-70f8d3ba7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI интерфейс\n",
    "async def chat_loop():\n",
    "    def format_response(response: Answer) -> str:\n",
    "        try:\n",
    "            return orjson.dumps(\n",
    "                response.model_dump(),\n",
    "                default=lambda x: (\n",
    "                    float(x) if isinstance(x, (np.integer, np.floating))\n",
    "                    else x.tolist() if isinstance(x, np.ndarray)\n",
    "                    else x\n",
    "                ),\n",
    "                option=orjson.OPT_INDENT_2\n",
    "            ).decode('utf-8')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error formatting response: {e}\")\n",
    "            return str(response.model_dump())\n",
    "\n",
    "    async def handle_user_feedback(question: str) -> None:\n",
    "        clarifying_question = await generate_clarifying_question(question)\n",
    "        print(\"\\nClarifying question:\")\n",
    "        print(clarifying_question)\n",
    "        \n",
    "        if clarifying_question != \"No clarification needed\":\n",
    "            follow_up = input(\"\\nWould you like to ask a clarifying question? (y/n): \").strip().lower()\n",
    "            if follow_up == \"y\":\n",
    "                new_question = input(\"\\nEnter clarified question: \").strip()\n",
    "                if new_question:\n",
    "                    response = await ask_question(new_question)\n",
    "                    print_response(response)\n",
    "\n",
    "    def print_response(response: Answer) -> None:\n",
    "        print(\"\\nStructured response:\")\n",
    "        print(format_response(response))\n",
    "        \n",
    "        if response.brief_answer:\n",
    "            print(\"\\nBrief answer:\")\n",
    "            print(response.brief_answer)\n",
    "            \n",
    "        if response.detailed_answer:\n",
    "            print(\"\\nDetailed answer:\")\n",
    "            print(response.detailed_answer)\n",
    "\n",
    "    print(\"CLI Chat Interface (type 'exit' to quit)\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\nYour question: \").strip()\n",
    "            \n",
    "            if not question:\n",
    "                print(\"Please enter a question.\")\n",
    "                continue\n",
    "                \n",
    "            if question.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "\n",
    "            response = await ask_question(question)\n",
    "            print_response(response)\n",
    "\n",
    "            satisfaction = input(\"\\nAre you satisfied with the answer? (y/n): \").strip().lower()\n",
    "            if satisfaction == \"n\":\n",
    "                await handle_user_feedback(question)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in chat loop: {str(e)}\")\n",
    "            print(f\"\\nError occurred: {str(e)}\")\n",
    "\n",
    "await chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
